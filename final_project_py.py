# -*- coding: utf-8 -*-
"""Final project.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HSn_zxD8ZLzDIHeF7EWto-khInutnBPo
"""

url="/content/guvidataset.csv"

import pandas as pd
df=pd.read_csv(url)

df.head()

df["label"].unique()

df.shape

df_text=df[["response"]]
df_text.head()

df_text['response']=df_text['response'].str.lower()
df_text.head()

def remove_whitespace(text):
    return text.strip()

df_text['response']=df_text['response'].apply(remove_whitespace)

import nltk
nltk.download('punkt')
nltk.download('punkt_tab')

from nltk import word_tokenize

df_text['response']=df_text['response'].apply(lambda X: word_tokenize(X))
df_text.head()

df_text["response"][0]

len(df_text["response"][0])

!pip install pyspellchecker

from spellchecker import SpellChecker

def spell_check(text):
  try:
    result = []
    spell = SpellChecker()
    for word in text:
      correct_word = spell.correction(word)
      result.append(correct_word)
    # return " ".join(result)
    return result
  except:
    return""

!pip install contextualSpellCheck

import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords
print(stopwords.words('english'))

from nltk.corpus import stopwords
print(stopwords.words('french'))

en_stopwords = stopwords.words('english')

def remove_stopwords(text):
  result = []
  for token in text:
    # token= token.lower()
    if token not in en_stopwords:
      result.append(token)
  return result



#Test
text = "this is the only solution of that question".split()
remove_stopwords(text)

df_text['response'] = df_text['response'].apply(remove_stopwords)
df_text.head()

from nltk import FreqDist

def frequent_words(df):

    lst=[]
    for text in df.values:
        lst+=text[0]
    fdist=FreqDist(lst)
    return fdist.most_common(20)

frequent_words(df_text)

freq_words = frequent_words(df_text)
print(len(freq_words))

lst = []
for a,b in freq_words:
  lst.append(b)

def remove_freq_words(text):
  result=[]
  for item in text:
    if item not in lst:
      result.append(item)
  return result

df_text['response']=df_text['response'].apply(remove_freq_words)

from nltk.stem import WordNetLemmatizer
from nltk import word_tokenize,pos_tag
import nltk
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger_eng')

def lemmatization(text):
  result=[]
  wordnet = WordNetLemmatizer()
  for token,tag in pos_tag(text):
    # print(token)
    # print(tag)
    pos=tag[0].lower()
    if pos not in ['a', 'r', 'n', 'v']:
      pos='n'
    result.append(wordnet.lemmatize(token,pos))
  return result

import nltk
nltk.download('omw-1.4')

from nltk.stem import PorterStemmer

import re
def remove_tag(text):


    text=' '.join(text)
    html_pattern = re.compile('<.*?>')
    return html_pattern.sub(r'', text)


#Test
text = "<HEAD> this is head tag </HEAD> head"
remove_tag(text.split())

df_text['response'] = df_text['response'].apply(remove_tag)
df_text.head()

import re
def remove_urls(text):
    url_pattern = re.compile(r'https?://\S+|www\.\S+')
    return url_pattern.sub(r'', text)

df_text['response'] = df_text['response'].apply(remove_urls)
df_text.head()

import pandas as pd
import numpy as np
import collections

import pandas as pd
# Local directory
guvi_data = pd.read_csv('//content/guvidataset.csv')

guvi_data.shape

guvi_data.head()

count = guvi_data.isnull().sum().sort_values(ascending=False)
percentage = ((guvi_data.isnull().sum()/len(guvi_data)*100)).sort_values(ascending=False)
missing_data = pd.concat([count, percentage], axis=1,
keys=['Count','Percentage'])

print('Count and percentage of missing values for the columns:')

missing_data

# Commented out IPython magic to ensure Python compatibility.
### Checking for the Distribution of Default ###
import matplotlib.pyplot as plt
# %matplotlib inline
print('Percentage for default\n')
print(round(guvi_data.label.value_counts(normalize=True)*100,2))
round(guvi_data.label.value_counts(normalize=True)*100,2).plot(kind='bar')
plt.title('Percentage Distributions by review type')
plt.show()

from sklearn.model_selection import train_test_split

Independent_var = guvi_data.response
Dependent_var = guvi_data.label

IV_train, IV_test, DV_train, DV_test = train_test_split(Independent_var, Dependent_var, test_size = 0.2, random_state = 225)

print('IV_train :', len(IV_train))
print('IV_test  :', len(IV_test))
print('DV_train :', len(DV_train))
print('DV_test  :', len(DV_test))

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

tvec = TfidfVectorizer()
clf2 = LogisticRegression(max_iter=10000)

from sklearn.metrics import confusion_matrix

# Transform the training data using the TfidfVectorizer
IV_train_transformed = tvec.fit_transform(IV_train)

# Train the Logistic Regression model
clf2.fit(IV_train_transformed, DV_train)

# Transform the test data using the fitted TfidfVectorizer
IV_test_transformed = tvec.transform(IV_test)

# Make predictions on the test data
predictions = clf2.predict(IV_test_transformed)

confusion_matrix(predictions, DV_test)

from sklearn.metrics import accuracy_score, precision_score, recall_score

print("Accuracy : ", accuracy_score(predictions, DV_test))
print("Precision : ", precision_score(predictions, DV_test, average = 'weighted'))
print("Recall : ", recall_score(predictions, DV_test, average = 'weighted'))

!pip install langdetect

### File: lang_detect.py
from langdetect import detect

def detect_language(text):
    try:
        return detect(text)
    except:
        return "en","urdu","ta","hindi","telugu","malyalam","arabic"

detect_language("weather is beautiful")

"""#DEEP LEARNING

"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd

# Load the dataset (assuming it's already loaded in 'guvi_data' from previous steps)
# If not loaded, you would load it here, e.g., guvi_data = pd.read_csv('/content/guvidataset.csv')

# Select features (X) and labels (y)
# Assuming 'response' is the feature and 'label' is the target
X = guvi_data['response']
y = guvi_data['label']

# Convert labels to numerical format if they are not already (e.g., using LabelEncoder)
# This is necessary for most deep learning models
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)


# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features (only if your features are numerical)
# Since X is currently text, this step will be applied after text vectorization
# scaler = StandardScaler()
# X_train = scaler.fit_transform(X_train)
# X_test = scaler.transform(X_test)

# For text data, you would typically vectorize it after splitting
# using methods like TF-IDF, CountVectorizer, or embeddings before scaling or feeding to a deep learning model.

print("Data splitting and initial preparation complete.")
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

from google.colab import drive
drive.mount('/content/drive')

from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

class Perceptron:
    def __init__(self, input_dim):
        self.weights = np.random.randn(input_dim)
        self.bias = np.random.randn()

    def predict(self, X):
        linear_output = np.dot(X, self.weights) + self.bias
        return sigmoid(linear_output)

    def train(self, X, y, epochs, learning_rate):
        for epoch in range(epochs):
            for i in range(X.shape[0]):
                linear_output = np.dot(X[i], self.weights) + self.bias
                prediction = sigmoid(linear_output)

                # Compute the error
                error = y[i] - prediction

                # Update the weights and bias
                self.weights += learning_rate * error * X[i] * sigmoid_derivative(prediction)
                self.bias += learning_rate * error * sigmoid_derivative(prediction)

            if epoch % 100 == 0:
                predictions = self.predict(X)
                loss = np.mean((y - predictions) ** 2)
                print(f"Epoch {epoch}, Loss: {loss:.4f}")

# Vectorize the text data
tvec = TfidfVectorizer()
X_train_vectorized = tvec.fit_transform(X_train)
X_test_vectorized = tvec.transform(X_test)

# Initialize the perceptron with the correct input dimension
perceptron = Perceptron(input_dim=X_train_vectorized.shape[1])

# Now you can train the perceptron with the vectorized data
# perceptron.train(X_train_vectorized.toarray(), y_train, epochs=1000, learning_rate=0.1)

# Train the perceptron
epochs = 1000
learning_rate = 0.01
perceptron.train(X_train_vectorized.toarray(), y_train, epochs, learning_rate)

def evaluate(model, X, y):
    predictions = model.predict(X)
    predictions = np.round(predictions)
    accuracy = np.mean(predictions == y) * 100
    return accuracy

# Evaluate the perceptron
train_accuracy = evaluate(perceptron, X_train_vectorized.toarray(), y_train)
test_accuracy = evaluate(perceptron, X_test_vectorized.toarray(), y_test)

print(f"Training Accuracy: {train_accuracy:.2f}%")
print(f"Testing Accuracy: {test_accuracy:.2f}%")

import pickle

# Save the model to a file using Pickle
with open('perceptron_model.pkl', 'wb') as file:
    pickle.dump(perceptron, file)

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer # Import TfidfVectorizer
import pandas as pd

# Assuming 'guvi_data' DataFrame is already loaded from previous steps
# If not, you would load it here:
# guvi_data = pd.read_csv('/content/guvidataset.csv')

# Select features (X) and labels (y)
X = guvi_data['response']
y = guvi_data['label']

# Convert labels to numerical format
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Split the dataset into training and testing sets
X_train, X_test, y_train_encoded, y_test_encoded = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

# Vectorize the text data
# Standardize the features (only if your features are numerical)
# Since X is text, we first vectorize and then can optionally scale the result.
# TF-IDF values are already somewhat scaled, so StandardScaler might not be strictly necessary after TF-IDF.
tvec = TfidfVectorizer()
X_train_vectorized = tvec.fit_transform(X_train)
X_test_vectorized = tvec.transform(X_test)

# Convert vectorized data and labels to PyTorch tensors
# Use .toarray() to convert sparse matrix to dense array for StandardScaler if used, or directly for tensors
X_train_tensor = torch.tensor(X_train_vectorized.toarray(), dtype=torch.float32)
X_test_tensor = torch.tensor(X_test_vectorized.toarray(), dtype=torch.float32)
y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.long)
y_test_tensor = torch.tensor(y_test_encoded, dtype=torch.long)

print("Data preparation for PyTorch complete.")
print("X_train_tensor shape:", X_train_tensor.shape)
print("X_test_tensor shape:", X_test_tensor.shape)
print("y_train_tensor shape:", y_train_tensor.shape)
print("y_test_tensor shape:", y_test_tensor.shape)

class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        # Input dimension should match the number of features in vectorized data (851)
        # Output dimension should match the number of unique labels (6)
        self.fc1 = nn.Linear(X_train_tensor.shape[1], 128)  # Input layer -> Hidden layer
        self.fc2 = nn.Linear(128, 6)  # Hidden layer -> Output layer

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Initialize the neural network
net = SimpleNN()

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)

# Training the network
epochs = 10
for epoch in range(epochs):
    # Forward pass
    outputs = net(X_train_tensor)
    loss = criterion(outputs, y_train_tensor)

    # Backward pass and optimize
    optimizer.zero_grad()  # Zero the gradients
    loss.backward()  # Backpropagate the loss
    optimizer.step()  # Update the weights


    if (epoch + 1) % 10 == 0:
        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')

print('Finished Training')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import warnings
from sklearn.metrics import mean_absolute_error
from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout,Flatten
warnings.filterwarnings("ignore")

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer # Import TfidfVectorizer
import pandas as pd

# Assuming 'guvi_data' DataFrame is already loaded from previous steps
# If not, you would load it here:
# guvi_data = pd.read_csv('/content/guvidataset.csv')

# Select features (X) and labels (y)
X = guvi_data['response']
y = guvi_data['label']

# Convert labels to numerical format
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Split the dataset into training and testing sets
X_train, X_test, y_train_encoded, y_test_encoded = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

# Vectorize the text data
# Standardize the features (only if your features are numerical)
# Since X is text, we first vectorize and then can optionally scale the result.
# TF-IDF values are already somewhat scaled, so StandardScaler might not be strictly necessary after TF-IDF.
tvec = TfidfVectorizer()
X_train_vectorized = tvec.fit_transform(X_train)
X_test_vectorized = tvec.transform(X_test)

# Convert vectorized data and labels to PyTorch tensors
# Use .toarray() to convert sparse matrix to dense array for StandardScaler if used, or directly for tensors
X_train_tensor = torch.tensor(X_train_vectorized.toarray(), dtype=torch.float32)
X_test_tensor = torch.tensor(X_test_vectorized.toarray(), dtype=torch.float32)
y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.long)
y_test_tensor = torch.tensor(y_test_encoded, dtype=torch.long)

print("Data preparation for PyTorch complete.")
print("X_train_tensor shape:", X_train_tensor.shape)
print("X_test_tensor shape:", X_test_tensor.shape)
print("y_train_tensor shape:", y_train_tensor.shape)
print("y_test_tensor shape:", y_test_tensor.shape)



class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(4, 10)  # Input layer (4 nodes) -> Hidden layer (10 nodes)
        self.fc2 = nn.Linear(10, 3)  # Hidden layer (10 nodes) -> Output layer (3 nodes)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Initialize the neural network
net = SimpleNN()

from datasets import load_dataset

# Login using e.g. `huggingface-cli login` to access this dataset
ds = load_dataset("HuggingFaceH4/Multilingual-Thinking")

# translator.py
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
from functools import lru_cache

# Use the distilled NLLB model for multi-language translation
NLLB_MODEL = "facebook/nllb-200-distilled-600M"

@lru_cache(maxsize=2)
def load_translator():
    tokenizer = AutoTokenizer.from_pretrained(NLLB_MODEL, src_lang="eng_Latn", tgt_lang="eng_Latn")
    model = AutoModelForSeq2SeqLM.from_pretrained(NLLB_MODEL)
    translator = pipeline("translation", model=model, tokenizer=tokenizer, device=0 if __import__("torch").cuda.is_available() else -1)
    return translator

def translate(text: str, src_lang_code: str, tgt_lang_code: str) -> str:
    """
    Translate text from src_lang_code to tgt_lang_code using NLLB.
    Language codes are NLLB codes like 'eng_Latn', 'tam_Taml', 'hin_Deva', etc.
    """
    translator = load_translator()
    # pipeline takes src_lang and tgt_lang args for NLLB
    out = translator(text, src_lang=src_lang_code, tgt_lang=tgt_lang_code)
    return out[0]["translation_text"]

import pandas as pd
from datasets import Dataset

# Load
df = pd.read_csv("/content/guvidataset.csv").dropna(subset=["prompt","response"])

# Merge to training text
df["text"] = "Q: " + df["prompt"] + "\nA: " + df["response"]

# Save cleaned CSV
df.to_csv("guvi_dataset_clean.csv", index=False)

# Optional: convert to HF Dataset
dataset = Dataset.from_pandas(df[["text"]])
dataset.save_to_disk("guvi_dataset_hf")

import os
os.environ["WANDB_MODE"] = "disabled"

from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling

# Load pre-trained model and tokenizer
model_name = "gpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

# Create dataset
def load_dataset(file_path, tokenizer, block_size=128):
    return TextDataset(
        tokenizer=tokenizer,
        file_path=file_path,
        block_size=block_size,
    )

train_dataset = load_dataset("/guvidataset.csv", tokenizer)

# Define training arguments
training_args = TrainingArguments(
    output_dir="./results",
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=4,
    save_steps=10_000,
    save_total_limit=2,
    # run_name=
)

# Initialize data collator
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_dataset,
)

# Fine-tune the model
trainer.train()

# Save the fine-tuned model and tokenizer
model.save_pretrained("./fine_tuned_model")
tokenizer.save_pretrained("./fine_tuned_model")

from transformers import pipeline

translator_en = pipeline("translation", model="Helsinki-NLP/opus-mt-mul-en")
translator_back = pipeline("translation", model="Helsinki-NLP/opus-mt-en-mul")

def translate_to_en(text):
    return translator_en(text)[0]['translation_text']

def translate_from_en(text, target_lang="hi"): # example Hindi
    model_name = f"Helsinki-NLP/opus-mt-en-{target_lang}"
    tr = pipeline("translation", model=model_name)
    return tr(text)[0]['translation_text']

!pip install streamlit

import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from translator import translate_to_en, translate_from_en

# Load fine-tuned GUVI GPT
model_name = "/content/fine_tuned_model"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

@st.cache_resource
def get_generator():
    return pipeline("text-generation", model=model, tokenizer=tokenizer)

generator = get_generator()

st.title("🌐 GUVI Multilingual Chatbot")
user_lang = st.selectbox("Choose your language", ["en", "hi", "ta", "te", "fr"])
user_input = st.text_area("Your message:")

if st.button("Send"):
    text_en = translate_to_en(user_input) if user_lang != "en" else user_input
    output = generator(text_en, max_length=150, num_return_sequences=1)[0]['generated_text']
    final = translate_from_en(output, target_lang=user_lang) if user_lang != "en" else output
    st.write("**Bot:**", final)

# Commented out IPython magic to ensure Python compatibility.
# %%writefile translator.py
# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
# from functools import lru_cache
# 
# # Use the distilled NLLB model for multi-language translation
# NLLB_MODEL = "facebook/nllb-200-distilled-600M"
# 
# @lru_cache(maxsize=2)
# def load_translator():
#     tokenizer = AutoTokenizer.from_pretrained(NLLB_MODEL, src_lang="eng_Latn", tgt_lang="eng_Latn")
#     model = AutoModelForSeq2SeqLM.from_pretrained(NLLB_MODEL)
#     translator = pipeline("translation", model=model, tokenizer=tokenizer, device=0 if __import__("torch").cuda.is_available() else -1)
#     return translator
# 
# def translate(text: str, src_lang_code: str, tgt_lang_code: str) -> str:
#     """
#     Translate text from src_lang_code to tgt_lang_code using NLLB.
#     Language codes are NLLB codes like 'eng_Latn', 'tam_Taml', 'hin_Deva', etc.
#     """
#     translator = load_translator()
#     # pipeline takes src_lang and tgt_lang args for NLLB
#     out = translator(text, src_lang=src_lang_code, tgt_lang=tgt_lang_code)
#     return out[0]["translation_text"]
# 
# # Add the translate_to_en and translate_from_en functions here as well
# def translate_to_en(text):
#     # Assuming the source language is automatically detected by the pipeline or you have a detection mechanism
#     # For simplicity, let's assume the input text's language needs to be detected first or is known.
#     # Since the NLLB pipeline itself takes src_lang and tgt_lang, we can use the translate function directly.
#     # However, the original code used Helsinki-NLP models for these functions, let's replicate that.
#     # We'll need to load those models here if they are not already loaded in the pipeline function above.
#     # For now, let's adapt the existing translate function assuming the source language is detected or provided elsewhere.
#     # A more robust solution would involve language detection first.
#     # For this fix, let's assume the input text is in some source language and we want to translate to English ('eng_Latn')
#     # This requires knowing the source language code.
# 
#     # *** NOTE: The original code for translate_to_en and translate_from_en in cell UPcllxGC1fcD used different models (Helsinki-NLP).
#     # To make the import work, I will include these functions here, but they might rely on different models than the NLLB one above.
#     # I will add placeholder implementations based on the Helsinki-NLP models used in cell UPcllxGC1fcD.
#     # You might need to adjust the model loading if you want to use NLLB for these specific functions.
# 
#     # Placeholder implementation based on Helsinki-NLP/opus-mt-mul-en
#     translator_en = pipeline("translation", model="Helsinki-NLP/opus-mt-mul-en")
#     return translator_en(text)[0]['translation_text']
# 
# 
# def translate_from_en(text, target_lang="hi"): # example Hindi
#     # Placeholder implementation based on Helsinki-NLP/opus-mt-en-mul
#     model_name = f"Helsinki-NLP/opus-mt-en-{target_lang}"
#     # Need to handle potential errors if the target language model doesn't exist
#     try:
#         tr = pipeline("translation", model=model_name)
#         return tr(text)[0]['translation_text']
#     except Exception as e:
#         print(f"Error translating to {target_lang}: {e}")
#         return f"Translation to {target_lang} failed."

